{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4a2c5f1",
   "metadata": {},
   "source": [
    "<!-- omit in toc -->\n",
    "# LangChain AI Agents Using Contextual Engineering\n",
    "\n",
    "Context engineering means creating the right setup for an AI before giving it a task. This setup includes:\n",
    "\n",
    "*   **Instructions** on how the AI should act, like being a helpful budget travel guide\n",
    "*   Access to **useful info** from databases, documents, or live sources.\n",
    "*   Remembering **past conversations** to avoid repeats or forgetting.\n",
    "*   **Tools** the AI can use, such as calculators or search features.\n",
    "*   Important details about you, like your **preferences** or location.\n",
    "\n",
    "![Context Engineering](https://cdn-images-1.medium.com/max/1500/1*sCTOzjG6KP7slQuxLZUtNg.png)\n",
    "*Context Engineering (From [LangChain](https://blog.langchain.com/context-engineering-for-agents/) and [12Factor](https://github.com/humanlayer/12-factor-agents/tree/main))*\n",
    "\n",
    "[AI engineers are now shifting](https://diamantai.substack.com/p/why-ai-experts-are-moving-from-prompt) from prompt engineering to context engineering because…\n",
    "\n",
    "> context engineering focuses on providing AI with the right background and tools, making its answers smarter and more useful.\n",
    "\n",
    "In this notebook, we will explore how **LangChain** and **LangGraph**, two powerful tools for building AI agents, RAG apps, and LLM apps, can be used to implement **contextual engineering** effectively to improve our AI Agents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b1d2c3",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "- [What is Context Engineering?](#what-is-context-engineering)\n",
    "- [Writing Context: Scratchpad and Memory](#writing-context-scratchpad-and-memory)\n",
    "- [Selecting Context: State, Memory, RAG, and Tools](#selecting-context-state-memory-rag-and-tools)\n",
    "- [Compressing Context: Summarization Strategies](#compressing-context-summarization-strategies)\n",
    "- [Isolating Context: Sub-Agents and Sandboxing](#isolating-context-sub-agents-and-sandboxing)\n",
    "- [Summarizing Everything](#summarizing-everything)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5g6h7i8",
   "metadata": {},
   "source": [
    "### What is Context Engineering?\n",
    "LLMs work like a new type of operating system. The LLM acts like the CPU, and its context window works like RAM, serving as its short-term memory. But, like RAM, the context window has limited space for different information.\n",
    "\n",
    "> Just as an operating system decides what goes into RAM, “context engineering” is about choosing what the LLM should keep in its context.\n",
    "\n",
    "![Different Context Types](https://cdn-images-1.medium.com/max/1000/1*kMEQSslFkhLiuJS8-WEMIg.png)\n",
    "\n",
    "When building LLM applications, we need to manage different types of context. Context engineering covers these main types:\n",
    "\n",
    "*   Instructions: prompts, examples, memories, and tool descriptions\n",
    "*   Knowledge: facts, stored information, and memories\n",
    "*   Tools: feedback and results from tool calls\n",
    "\n",
    "This year, more people are interested in agents because LLMs are better at thinking and using tools. Agents work on long tasks by using LLMs and tools together, choosing the next step based on the tool’s feedback.\n",
    "\n",
    "![Agent Workflow](https://cdn-images-1.medium.com/max/1500/1*Do44CZkpPYyIJefuNQ69GA.png)\n",
    "\n",
    "But long tasks and collecting too much feedback from tools use a lot of tokens. This can create problems: the context window can overflow, costs and delays can increase, and the agent might work worse.\n",
    "\n",
    "Drew Breunig explained how too much context can hurt performance, including:\n",
    "\n",
    "*   Context Poisoning: [when a mistake or hallucination gets added to the context](https://www.dbreunig.com/2025/06/22/how-contexts-fail-and-how-to-fix-them.html?ref=blog.langchain.com#context-poisoning)\n",
    "*   Context Distraction: [when too much context confuses the model](https://www.dbreunig.com/2025/06/22/how-contexts-fail-and-how-to-fix-them.html?ref=blog.langchain.com#context-distraction)\n",
    "*   Context Confusion: [when extra, unnecessary details affect the answer](https://www.dbreunig.com/2025/06/22/how-contexts-fail-and-how-to-fix-them.html?ref=blog.langchain.com#context-confusion)\n",
    "*   Context Clash: [when parts of the context give conflicting information](https://www.dbreunig.com/2025/06/22/how-contexts-fail-and-how-to-fix-them.html?ref=blog.langchain.com#context-clash)\n",
    "\n",
    "![Multiple turns in Agent](https://cdn-images-1.medium.com/max/1500/1*ZJeZJPKI5jC_1BMCoghZxA.png)\n",
    "\n",
    "Anthropic [in their research](https://www.anthropic.com/engineering/built-multi-agent-research-system?ref=blog.langchain.com) stressed the need for it:\n",
    "\n",
    "> Agents often have conversations with hundreds of turns, so managing context carefully is crucial.\n",
    "\n",
    "So, how are people solving this problem today? Common strategies for agent context engineering can be grouped into four main types:\n",
    "\n",
    "*   **Write**: creating clear and useful context\n",
    "*   **Select**: picking only the most relevant information\n",
    "*   **Compress**: shortening context to save space\n",
    "*   **Isolate**: keeping different types of context separate\n",
    "\n",
    "![Categories of Context Engineering](https://cdn-images-1.medium.com/max/2600/1*CacnXVAI6wR4eSIWgnZ9sg.png)\n",
    "*Categories of Context Engineering (From [LangChain docs](https://blog.langchain.com/context-engineering-for-agents/))*\n",
    "\n",
    "[LangGraph](https://www.langchain.com/langgraph) is built to support all these strategies. We will go through each of these components one by one and see how they help make our AI agents work better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j9k1l2m3",
   "metadata": {},
   "source": [
    "### Writing Context: Scratchpad and Memory\n",
    "\n",
    "The first principle of contextual engineering is **writing** context. This means creating and storing information outside the LLM's immediate context window, which the agent can access later. We will explore two primary mechanisms for this in LangGraph: the **scratchpad** (for short-term, session-specific notes) and **memory** (for long-term persistence across sessions).\n",
    "\n",
    "![First Component of CE](https://cdn-images-1.medium.com/max/1000/1*aXpKxYt03iZPcrGkxsFvrQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n4o5p6q7",
   "metadata": {},
   "source": [
    "#### Scratchpad with LangGraph\n",
    "Just like humans take notes to remember things for later tasks, agents can do the same using a [scratchpad](https://www.anthropic.com/engineering/claude-think-tool). It stores information outside the context window so the agent can access it whenever needed.\n",
    "\n",
    "A good example is [Anthropic's multi-agent researcher](https://www.anthropic.com/engineering/built-multi-agent-research-system):\n",
    "\n",
    "> *The LeadResearcher plans its approach and saves it to memory, because if the context window goes beyond 200,000 tokens, it gets cut off so saving the plan ensures it isn’t lost.*\n",
    "\n",
    "In LangGraph, the `StateGraph` object serves as this scratchpad. The state is the central data structure passed between nodes in your graph. You define its schema, and each node can read from and write to it. This provides a powerful way to maintain short-term, thread-scoped memory for your agent.\n",
    "\n",
    "First, let's set up our environment and helper utilities for printing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r8s9t0u1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for typing, formatting, and environment management\n",
    "import getpass\n",
    "import os\n",
    "from typing import TypedDict\n",
    "\n",
    "from IPython.display import Image, display\n",
    "from rich.console import Console\n",
    "from rich.pretty import pprint\n",
    "\n",
    "# Initialize a console for rich, formatted output in the notebook.\n",
    "console = Console()\n",
    "\n",
    "# Set the Anthropic API key to authenticate requests\n",
    "# It's recommended to set this as an environment variable for security\n",
    "if \"ANTHROPIC_API_KEY\" not in os.environ:\n",
    "    os.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass(\"Provide your Anthropic API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v2w3x4y5",
   "metadata": {},
   "source": [
    "Next, we will create a `TypedDict` for the state object. This defines the schema of our scratchpad, ensuring data consistency as it flows through the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z6a7b8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema for the graph's state using TypedDict.\n",
    "# This class acts as a data structure that will be passed between nodes in the graph.\n",
    "# It ensures that the state has a consistent shape and provides type hints.\n",
    "class State(TypedDict):\n",
    "    \"\"\"\n",
    "    Defines the structure of the state for our joke generator workflow.\n",
    "\n",
    "    Attributes:\n",
    "        topic: The input topic for which a joke will be generated.\n",
    "        joke: The output field where the generated joke will be stored.\n",
    "    \"\"\"\n",
    "\n",
    "    topic: str\n",
    "    joke: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e1f2g3",
   "metadata": {},
   "source": [
    "#### Creating a StateGraph to Write to the Scratchpad\n",
    "Once we define a state object, we can write context to it using a `StateGraph`. A StateGraph is LangGraph’s main tool for building stateful agents.\n",
    "\n",
    "- **Nodes** are steps in the workflow. Each node is a function that takes the current state as input and returns updates.\n",
    "- **Edges** connect nodes, defining the execution flow.\n",
    "\n",
    "Let's create a chat model and a node function that uses it to generate a joke and write it to our state object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h4i5j6k7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for LangChain and LangGraph\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "\n",
    "# --- Model Setup ---\n",
    "# Initialize the chat model to be used in the workflow\n",
    "# We use a specific Claude model with temperature=0 for deterministic outputs\n",
    "llm = init_chat_model(\"anthropic:claude-3-sonnet-20240229\", temperature=0)\n",
    "\n",
    "# --- Define Workflow Node ---\n",
    "def generate_joke(state: State) -> dict[str, str]:\n",
    "    \"\"\"\n",
    "    A node function that generates a joke based on the topic in the current state.\n",
    "\n",
    "    This function reads the 'topic' from the state, uses the LLM to generate a joke,\n",
    "    and returns a dictionary to update the 'joke' field in the state.\n",
    "\n",
    "    Args:\n",
    "        state: The current state of the graph, which must contain a 'topic'.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with the 'joke' key to update the state.\n",
    "    \"\"\"\n",
    "    # Read the topic from the state\n",
    "    topic = state[\"topic\"]\n",
    "    print(f\"Generating a joke about: {topic}\")\n",
    "\n",
    "    # Invoke the language model to generate a joke\n",
    "    msg = llm.invoke(f\"Write a short joke about {topic}\")\n",
    "\n",
    "    # Return the generated joke to be written back to the state\n",
    "    return {\"joke\": msg.content}\n",
    "\n",
    "# --- Build and Compile the Graph ---\n",
    "# Initialize a new StateGraph with the predefined State schema\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# Add the 'generate_joke' function as a node in the graph\n",
    "workflow.add_node(\"generate_joke\", generate_joke)\n",
    "\n",
    "# Define the workflow's execution path:\n",
    "# The graph starts at the START entrypoint and flows to our 'generate_joke' node.\n",
    "workflow.add_edge(START, \"generate_joke\")\n",
    "# After 'generate_joke' completes, the graph execution ends.\n",
    "workflow.add_edge(\"generate_joke\", END)\n",
    "\n",
    "# Compile the workflow into an executable chain\n",
    "chain = workflow.compile()\n",
    "\n",
    "# --- Visualize the Graph ---\n",
    "# Display a visual representation of the compiled workflow graph\n",
    "display(Image(chain.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l8m9n0o1",
   "metadata": {},
   "source": [
    "Now we can execute this workflow. It will take an initial state with a `topic`, run the `generate_joke` node, and write the result into the `joke` field of the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p2q3r4s5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Execute the Workflow ---\n",
    "# Invoke the compiled graph with an initial state containing the topic.\n",
    "# The `invoke` method runs the graph from the START node to the END node.\n",
    "joke_generator_state = chain.invoke({\"topic\": \"cats\"})\n",
    "\n",
    "# --- Display the Final State ---\n",
    "# Print the final state of the graph after execution.\n",
    "# This will show both the input 'topic' and the output 'joke' that was written to the state.\n",
    "console.print(\"\\n[bold blue]Joke Generator Final State:[/bold blue]\")\n",
    "pprint(joke_generator_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t6u7v8w9",
   "metadata": {},
   "source": [
    "#### Memory Writing in LangGraph\n",
    "Scratchpads help agents work within a single session, but sometimes agents need to remember things across multiple sessions. This is where long-term memory comes in.\n",
    "\n",
    "*   [Reflexion](https://arxiv.org/abs/2303.11366) introduced the idea of agents reflecting after each turn and reusing self-generated hints.\n",
    "*   [Generative Agents](https://ar5iv.labs.arxiv.org/html/2304.03442) created long-term memories by summarizing past agent feedback.\n",
    "\n",
    "![Memory Writing](https://cdn-images-1.medium.com/max/1000/1*VaMVevdSVxDITLK1j0LfRQ.png)\n",
    "\n",
    "LangGraph supports long-term memory through a `store` that can be passed to a compiled graph. This allows you to persist context *across threads* (e.g., different chat sessions).\n",
    "\n",
    "- **Checkpointing** saves the graph’s state at each step in a `thread`.\n",
    "- **Long-term memory** lets you keep specific context across threads using a key-value `BaseStore`.\n",
    "\n",
    "Let's enhance our agent to use both short-term checkpointing and a long-term memory store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x0y1z2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import memory and persistence components from LangGraph\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.store.base import BaseStore\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "# Initialize storage components\n",
    "checkpointer = InMemorySaver()  # For thread-level state persistence (short-term memory)\n",
    "memory_store = InMemoryStore()  # For cross-thread memory storage (long-term memory)\n",
    "\n",
    "# Define a namespace to logically group related data in the long-term store.\n",
    "namespace = (\"rlm\", \"joke_generator\")\n",
    "\n",
    "def generate_joke_with_memory(state: State, store: BaseStore) -> dict[str, str]:\n",
    "    \"\"\"Generate a joke with memory awareness.\n",
    "    \n",
    "    This enhanced version checks for existing jokes in long-term memory\n",
    "    before generating a new one and saves the new joke.\n",
    "    \n",
    "    Args:\n",
    "        state: Current state containing the topic.\n",
    "        store: Memory store for persistent context.\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary with the generated joke.\n",
    "    \"\"\"\n",
    "    # Check if there's an existing joke in memory (we will cover selection later)\n",
    "    existing_jokes = list(store.search(namespace))\n",
    "    if existing_jokes:\n",
    "        existing_joke_content = existing_jokes[0].value\n",
    "        print(f\"Found existing joke in memory: {existing_joke_content}\")\n",
    "    else:\n",
    "        print(\"No existing joke found in memory.\")\n",
    "\n",
    "    # Generate a new joke based on the topic\n",
    "    msg = llm.invoke(f\"Write a short joke about {state['topic']}\")\n",
    "    \n",
    "    # Write the new joke to long-term memory\n",
    "    store.put(namespace, \"last_joke\", {\"joke\": msg.content})\n",
    "    print(f\"Wrote new joke to memory: {msg.content[:50]}...\")\n",
    "\n",
    "    # Return the joke to be added to the current session's state (scratchpad)\n",
    "    return {\"joke\": msg.content}\n",
    "\n",
    "\n",
    "# Build the workflow with memory capabilities\n",
    "workflow_with_memory = StateGraph(State)\n",
    "workflow_with_memory.add_node(\"generate_joke\", generate_joke_with_memory)\n",
    "workflow_with_memory.add_edge(START, \"generate_joke\")\n",
    "workflow_with_memory.add_edge(\"generate_joke\", END)\n",
    "\n",
    "# Compile with both checkpointing (for session state) and a memory store (for long-term)\n",
    "chain_with_memory = workflow_with_memory.compile(checkpointer=checkpointer, store=memory_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c5d6e7",
   "metadata": {},
   "source": [
    "Now, let's execute the updated workflow. We'll use a `config` object to specify a `thread_id`. This identifies the current session. The first time we run it, there should be no joke in long-term memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8g9h0i1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the workflow within a specific thread (e.g., a user session)\n",
    "config_thread_1 = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "joke_state_thread_1 = chain_with_memory.invoke({\"topic\": \"dogs\"}, config_thread_1)\n",
    "\n",
    "# Display the workflow result for the first thread\n",
    "console.print(\"\\n[bold cyan]Workflow Result (Thread 1):[/bold cyan]\")\n",
    "pprint(joke_state_thread_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j2k3l4m5",
   "metadata": {},
   "source": [
    "Because we compiled the workflow with a checkpointer, we can now view the latest state of the graph for that thread. This shows the value of the short-term scratchpad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "n6o7p8q9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Retrieve and Inspect the Graph State ---\n",
    "# Use the `get_state` method to retrieve the latest state snapshot for thread \"1\".\n",
    "latest_state_thread_1 = chain_with_memory.get_state(config_thread_1)\n",
    "\n",
    "# --- Display the State Snapshot ---\n",
    "# The StateSnapshot includes not only the data ('topic', 'joke') but also execution metadata.\n",
    "console.print(\"\\n[bold magenta]Latest Graph State (Thread 1):[/bold magenta]\")\n",
    "pprint(latest_state_thread_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r0s1t2u3",
   "metadata": {},
   "source": [
    "Now, let's run the workflow again but with a *different* `thread_id`. This simulates a new session. Our long-term memory store should now contain the joke from the first session, demonstrating how context can be persisted and shared across threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v4w5x6y7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the workflow with a different thread ID to simulate a new session\n",
    "config_thread_2 = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "joke_state_thread_2 = chain_with_memory.invoke({\"topic\": \"birds\"}, config_thread_2)\n",
    "\n",
    "# Display the result, which should show that it found the joke from the previous thread in memory\n",
    "console.print(\"\\n[bold yellow]Workflow Result (Thread 2):[/bold yellow]\")\n",
    "pprint(joke_state_thread_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c2d3e4",
   "metadata": {},
   "source": [
    "### Selecting Context: State, Memory, RAG, and Tools\n",
    "\n",
    "The second principle is **selecting** context. Once context is written, agents need to be able to retrieve the *most relevant* pieces of information for the current task. This prevents context window overflow and keeps the agent focused.\n",
    "\n",
    "![Second Component of CE](https://cdn-images-1.medium.com/max/1000/1*VZiHtQ_8AlNdV3HIMrbBZA.png)\n",
    "\n",
    "We will explore four ways to select context:\n",
    "1.  **From the Scratchpad (State):** Selecting data written in the current session.\n",
    "2.  **From Long-Term Memory:** Retrieving data from past sessions.\n",
    "3.  **From Knowledge (RAG):** Using Retrieval-Augmented Generation to fetch information from documents.\n",
    "4.  **From Tools (Tool-RAG):** Using RAG to select the best tool for a job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5g6h7i8j9",
   "metadata": {},
   "source": [
    "#### Scratchpad Selection Approach\n",
    "How you select context from a scratchpad depends on its implementation. Since our scratchpad is the agent's runtime `State` object, we (the developer) decide which parts of the state to share with the agent at each step. This gives fine-grained control.\n",
    "\n",
    "Let's create a two-step workflow. The first node generates a joke (writes to state). The second node *selects* that joke from the state and improves it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k1l2m3n4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need a state that can hold the original and the improved joke\n",
    "class JokeImprovementState(TypedDict):\n",
    "    topic: str\n",
    "    joke: str\n",
    "    improved_joke: str\n",
    "\n",
    "def improve_joke(state: JokeImprovementState) -> dict[str, str]:\n",
    "    \"\"\"Improve an existing joke by adding wordplay.\n",
    "    \n",
    "    This demonstrates selecting context from state - we read the existing\n",
    "    joke from state and use it to generate an improved version.\n",
    "    \n",
    "    Args:\n",
    "        state: Current state containing the original joke.\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary with the improved joke.\n",
    "    \"\"\"\n",
    "    initial_joke = state[\"joke\"]\n",
    "    print(f\"Initial joke selected from state: {initial_joke[:50]}...\")\n",
    "    \n",
    "    # Select the joke from state to present it to the LLM\n",
    "    msg = llm.invoke(f\"Make this joke funnier by adding wordplay: {initial_joke}\")\n",
    "    return {\"improved_joke\": msg.content}\n",
    "\n",
    "# --- Build the two-step workflow ---\n",
    "selection_workflow = StateGraph(JokeImprovementState)\n",
    "\n",
    "# Add the initial joke generation node (reusing from before)\n",
    "selection_workflow.add_node(\"generate_joke\", generate_joke)\n",
    "# Add the new improvement node\n",
    "selection_workflow.add_node(\"improve_joke\", improve_joke)\n",
    "\n",
    "# Connect nodes in sequence\n",
    "selection_workflow.add_edge(START, \"generate_joke\")\n",
    "selection_workflow.add_edge(\"generate_joke\", \"improve_joke\")\n",
    "selection_workflow.add_edge(\"improve_joke\", END)\n",
    "\n",
    "# Compile the workflow\n",
    "selection_chain = selection_workflow.compile()\n",
    "\n",
    "# Visualize the new graph\n",
    "display(Image(selection_chain.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o5p6q7r8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the workflow to see context selection in action\n",
    "joke_improvement_state = selection_chain.invoke({\"topic\": \"computers\"})\n",
    "\n",
    "# Display the final state with rich formatting\n",
    "console.print(\"\\n[bold blue]Final Joke Improvement State:[/bold blue]\")\n",
    "pprint(joke_improvement_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s9t0u1v2",
   "metadata": {},
   "source": [
    "#### Memory Selection Ability\n",
    "If agents can save memories, they also need to select relevant memories for the task at hand. This is useful for recalling:\n",
    "- **Episodic memories:** Few-shot examples of desired behavior.\n",
    "- **Procedural memories:** Instructions to guide behavior.\n",
    "- **Semantic memories:** Facts or relationships for task-relevant context.\n",
    "\n",
    "In our previous example, we wrote to the `InMemoryStore`. Now, we can select context from it using the `store.get()` method to pull relevant state into our workflow. Let's create a node that selects the previously stored joke and tries to generate a *different* one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w3x4y5z6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialize storage components for this example\n",
    "checkpointer_select = InMemorySaver()\n",
    "memory_store_select = InMemoryStore()\n",
    "# Pre-populate the store with a joke for selection\n",
    "memory_store_select.put(namespace, \"last_joke\", {\"joke\": \"Why was the computer cold? Because it left its Windows open!\"})\n",
    "\n",
    "def generate_different_joke(state: State, store: BaseStore) -> dict[str, str]:\n",
    "    \"\"\"Generate a joke with memory-aware context selection.\n",
    "    \n",
    "    This function demonstrates selecting context from memory before\n",
    "    generating new content, ensuring it doesn't repeat itself.\n",
    "    \n",
    "    Args:\n",
    "        state: Current state containing the topic\n",
    "        store: Memory store for persistent context\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with the newly generated joke\n",
    "    \"\"\"\n",
    "    # Select prior joke from memory if it exists\n",
    "    prior_joke_item = store.get(namespace, \"last_joke\")\n",
    "    prior_joke_text = \"None\"\n",
    "    if prior_joke_item:\n",
    "        prior_joke_text = prior_joke_item.value[\"joke\"]\n",
    "        print(f\"Selected prior joke from memory: {prior_joke_text}\")\n",
    "    else:\n",
    "        print(\"No prior joke found in memory.\")\n",
    "\n",
    "    # Generate a new joke that differs from the prior one\n",
    "    prompt = (\n",
    "        f\"Write a short joke about {state['topic']}, \"\n",
    "        f\"but make it different from this prior joke: '{prior_joke_text}'\"\n",
    "    )\n",
    "    msg = llm.invoke(prompt)\n",
    "\n",
    "    # Store the new joke in memory for future context selection\n",
    "    store.put(namespace, \"last_joke\", {\"joke\": msg.content})\n",
    "\n",
    "    return {\"joke\": msg.content}\n",
    "\n",
    "# Build the memory-aware workflow\n",
    "memory_selection_workflow = StateGraph(State)\n",
    "memory_selection_workflow.add_node(\"generate_joke\", generate_different_joke)\n",
    "memory_selection_workflow.add_edge(START, \"generate_joke\")\n",
    "memory_selection_workflow.add_edge(\"generate_joke\", END)\n",
    "\n",
    "# Compile with both checkpointing and memory store\n",
    "memory_selection_chain = memory_selection_workflow.compile(checkpointer=checkpointer_select, store=memory_store_select)\n",
    "\n",
    "# Execute the workflow\n",
    "config = {\"configurable\": {\"thread_id\": \"3\"}}\n",
    "new_joke_state = memory_selection_chain.invoke({\"topic\": \"computers\"}, config)\n",
    "\n",
    "console.print(\"\\n[bold green]Memory Selection Workflow Final State:[/bold green]\")\n",
    "pprint(new_joke_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b8c9d0",
   "metadata": {},
   "source": [
    "#### Advantage of LangGraph BigTool Calling (Tool Selection)\n",
    "Agents use tools, but giving them too many can cause confusion, especially when tool descriptions overlap. A solution is to use RAG on tool descriptions to fetch only the most relevant tools for a task.\n",
    "\n",
    "> According to [recent research](https://arxiv.org/abs/2505.03275), this improves tool selection accuracy by up to 3x.\n",
    "\n",
    "The `langgraph-bigtool` library is ideal for this. It applies semantic similarity search over tool descriptions to select the most relevant ones. Let’s demonstrate by creating an agent with all functions from Python’s built-in `math` library and see how it selects the correct one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f2g3h4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for this example\n",
    "import math\n",
    "import types\n",
    "import uuid\n",
    "\n",
    "from langchain.embeddings import init_embeddings\n",
    "from langgraph_bigtool import create_agent\n",
    "from langgraph_bigtool.utils import convert_positional_only_function_to_tool\n",
    "from utils import format_messages # A helper from the provided utils.py\n",
    "\n",
    "# Ensure OpenAI API key is set for embeddings\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Provide your OpenAI API key: \")\n",
    "\n",
    "# --- 1. Collect and Prepare Tools ---\n",
    "# Collect all built-in functions from the `math` module\n",
    "all_math_tools = []\n",
    "for function_name in dir(math):\n",
    "    function = getattr(math, function_name)\n",
    "    if isinstance(function, types.BuiltinFunctionType):\n",
    "        # This handles an idiosyncrasy of the `math` library's function signatures\n",
    "        if tool := convert_positional_only_function_to_tool(function):\n",
    "            all_math_tools.append(tool)\n",
    "\n",
    "# Create a registry mapping unique IDs to each tool instance\n",
    "tool_registry = {str(uuid.uuid4()): tool for tool in all_math_tools}\n",
    "\n",
    "# --- 2. Index Tools for Semantic Search ---\n",
    "# Initialize the embeddings model\n",
    "embeddings = init_embeddings(\"openai:text-embedding-3-small\")\n",
    "\n",
    "# Set up an in-memory store configured for vector search on tool descriptions\n",
    "tool_store = InMemoryStore(\n",
    "    index={\n",
    "        \"embed\": embeddings,\n",
    "        \"dims\": 1536, # Dimension for text-embedding-3-small\n",
    "        \"fields\": [\"description\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Index each tool's name and description into the store\n",
    "for tool_id, tool in tool_registry.items():\n",
    "    tool_store.put(\n",
    "        (\"tools\",), # A namespace for tools\n",
    "        tool_id,\n",
    "        {\"description\": f\"{tool.name}: {tool.description}\"},\n",
    "    )\n",
    "\n",
    "# --- 3. Create and Compile the Agent ---\n",
    "# The create_agent function from langgraph-bigtool sets up the agent logic\n",
    "builder = create_agent(llm, tool_registry)\n",
    "bigtool_agent = builder.compile(store=tool_store)\n",
    "\n",
    "display(Image(bigtool_agent.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
    "id": "i5j6k7l8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Invoke the Agent ---\n",
    "# Define the query for the agent. This requires selecting the correct math tool.\n",
    "query = \"Use available tools to calculate arc cosine of 0.5.\"\n",
    "\n",
    "# Invoke the agent. It will first search its tools, select 'acos', and then execute it.\n",
    "result = bigtool_agent.invoke({\"messages\": query})\n",
    "\n",
    "# Format and display the final messages from the agent's execution.\n",
    "# The output will show the agent's thought process: searching, finding, and using the tool.\n",
    "format_messages(result['messages'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m9n0o1p2",
   "metadata": {},
   "source": [
    "#### RAG with Contextual Engineering (Knowledge Selection)\n",
    "[RAG (Retrieval-Augmented Generation)](https://github.com/langchain-ai/rag-from-scratch) is a cornerstone of context engineering. It allows agents to select relevant knowledge from vast document stores.\n",
    "\n",
    "In LangGraph, this is typically done by creating a retrieval tool. Let's build a RAG agent that can answer questions about Lilian Weng’s blog posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q3r4s5t6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary components for RAG\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langgraph.graph import MessagesState\n",
    "from langchain_core.messages import SystemMessage, ToolMessage\n",
    "from typing_extensions import Literal\n",
    "\n",
    "# --- 1. Load and Chunk Documents ---\n",
    "# Define the URLs for Lilian Weng's blog posts\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2025-05-01-thinking/\",\n",
    "    \"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\",\n",
    "    \"https://lilianweng.github.io/posts/2024-07-07-hallucination/\",\n",
    "    \"https://lilianweng.github.io/posts/2024-04-12-diffusion-video/\",\n",
    "]\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "# Split the documents into smaller chunks for effective retrieval\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=2000, chunk_overlap=50\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# --- 2. Create Vector Store and Retriever Tool ---\n",
    "vectorstore = InMemoryVectorStore.from_documents(documents=doc_splits, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Create a retriever tool that the agent can call\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"retrieve_blog_posts\",\n",
    "    \"Search and return information about Lilian Weng blog posts.\",\n",
    ")\n",
    "\n",
    "rag_tools = [retriever_tool]\n",
    "rag_tools_by_name = {tool.name: tool for tool in rag_tools}\n",
    "llm_with_rag_tools = llm.bind_tools(rag_tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u7v8w9x0",
   "metadata": {},
   "source": [
    "Now we define the graph components for our RAG agent: the prompt, the nodes for calling the LLM and the tool, and a conditional edge to create a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y1z2a3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Define the RAG Agent Graph ---\n",
    "rag_prompt = \"\"\"You are a helpful assistant tasked with retrieving information from a series of technical blog posts by Lilian Weng. \n",
    "Clarify the scope of research with the user before using your retrieval tool to gather context. Reflect on any context you fetch, and\n",
    "proceed until you have sufficient context to answer the user's research request.\"\"\"\n",
    "\n",
    "def rag_llm_call(state: MessagesState):\n",
    "    \"\"\"Node to call the LLM. The LLM decides whether to call a tool or generate a final answer.\"\"\"\n",
    "    messages_with_prompt = [SystemMessage(content=rag_prompt)] + state[\"messages\"]\n",
    "    response = llm_with_rag_tools.invoke(messages_with_prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def rag_tool_node(state: dict):\n",
    "    \"\"\"Node to perform the tool call and return the observation.\"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    result = []\n",
    "    for tool_call in last_message.tool_calls:\n",
    "        tool = rag_tools_by_name[tool_call[\"name\"]]\n",
    "        observation = tool.invoke(tool_call[\"args\"])\n",
    "        result.append(ToolMessage(content=str(observation), tool_call_id=tool_call[\"id\"]))\n",
    "    return {\"messages\": result}\n",
    "\n",
    "def should_continue_rag(state: MessagesState) -> Literal[\"Action\", END]:\n",
    "    \"\"\"Conditional edge to decide the next step. If the LLM made a tool call, route to the tool node. Otherwise, end.\"\"\"\n",
    "    if state[\"messages\"][-1].tool_calls:\n",
    "        return \"Action\"\n",
    "    return END\n",
    "\n",
    "# Build the RAG agent workflow\n",
    "rag_agent_builder = StateGraph(MessagesState)\n",
    "rag_agent_builder.add_node(\"llm_call\", rag_llm_call)\n",
    "rag_agent_builder.add_node(\"Action\", rag_tool_node)\n",
    "rag_agent_builder.set_entry_point(\"llm_call\")\n",
    "rag_agent_builder.add_conditional_edges(\"llm_call\", should_continue_rag, {\"Action\": \"Action\", END: END})\n",
    "rag_agent_builder.add_edge(\"Action\", \"llm_call\")\n",
    "\n",
    "rag_agent = rag_agent_builder.compile()\n",
    "display(Image(rag_agent.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d6e7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Invoke the RAG Agent ---\n",
    "query = \"What are the types of reward hacking discussed in the blogs?\"\n",
    "result = rag_agent.invoke({\"messages\": [(\"user\", query)]})\n",
    "format_messages(result['messages'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g9h0i1j2",
   "metadata": {},
   "source": [
    "### Compressing Context: Summarization Strategies\n",
    "\n",
    "The third principle is **compressing** context. Agent interactions can span hundreds of turns and involve token-heavy tool calls. Summarization is a common and effective way to manage this, reducing token count while retaining essential information.\n",
    "\n",
    "![Third Component of CE](https://cdn-images-1.medium.com/max/1000/1*Xu76qgF1u2G3JipeIgHo5Q.png)\n",
    "\n",
    "We can add summarization at different points in the agent's workflow:\n",
    "- At the end of a conversation to create a summary of the entire interaction.\n",
    "- After a token-heavy tool call to compress its output before it enters the agent's scratchpad.\n",
    "\n",
    "Let's explore both approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k3l4m5n6",
   "metadata": {},
   "source": [
    "#### Approach 1: Summarizing the Entire Conversation\n",
    "\n",
    "First, we'll build an agent that performs its RAG task and then, as a final step, generates a summary of the whole interaction. This can be useful for logging or creating a concise record of the agent's work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o7p8q9r0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.markdown import Markdown\n",
    "\n",
    "# Define an extended state that includes a summary field\n",
    "class StateWithSummary(MessagesState):\n",
    "    summary: str\n",
    "\n",
    "summarization_prompt = \"\"\"Summarize the full chat history and all tool feedback to give an overview of what the user asked about and what the agent did.\"\"\"\n",
    "\n",
    "def summary_node(state: MessagesState) -> dict:\n",
    "    \"\"\"Node to generate a summary of the conversation.\"\"\"\n",
    "    messages = [SystemMessage(content=summarization_prompt)] + state[\"messages\"]\n",
    "    result = llm.invoke(messages)\n",
    "    return {\"summary\": result.content}\n",
    "\n",
    "def should_continue_to_summary(state: MessagesState) -> Literal[\"Action\", \"summary_node\"]:\n",
    "    \"\"\"Conditional edge to route to tool action or to the final summary node.\"\"\"\n",
    "    if state[\"messages\"][-1].tool_calls:\n",
    "        return \"Action\"\n",
    "    return \"summary_node\"\n",
    "\n",
    "# Build the workflow with a final summary step\n",
    "summary_agent_builder = StateGraph(StateWithSummary)\n",
    "summary_agent_builder.add_node(\"llm_call\", rag_llm_call)\n",
    "summary_agent_builder.add_node(\"Action\", rag_tool_node)\n",
    "summary_agent_builder.add_node(\"summary_node\", summary_node)\n",
    "summary_agent_builder.set_entry_point(\"llm_call\")\n",
    "summary_agent_builder.add_conditional_edges(\"llm_call\", should_continue_to_summary, {\"Action\": \"Action\", \"summary_node\": \"summary_node\"})\n",
    "summary_agent_builder.add_edge(\"Action\", \"llm_call\")\n",
    "summary_agent_builder.add_edge(\"summary_node\", END)\n",
    "\n",
    "summary_agent = summary_agent_builder.compile()\n",
    "display(Image(summary_agent.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s1t2u3v4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the agent and display the final summary\n",
    "query = \"Why does RL improve LLM reasoning according to the blogs?\"\n",
    "result = summary_agent.invoke({\"messages\": [(\"user\", query)]})\n",
    "\n",
    "console.print(\"\\n[bold green]Final Agent Message:[/bold green]\")\n",
    "format_messages([result['messages'][-1]])\n",
    "\n",
    "console.print(\"\\n[bold purple]Generated Conversation Summary:[/bold purple]\")\n",
    "display(Markdown(result[\"summary\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w5x6y7z8",
   "metadata": {},
   "source": [
    "**Note:** While effective, this approach can be token-intensive, as the full, uncompressed tool outputs are passed through the loop. For the query above, this can use over 100k tokens.\n",
    "\n",
    "#### Approach 2: Compressing Tool Outputs On-the-Fly\n",
    "A more efficient approach is to compress the context *before* it enters the agent’s main scratchpad. Let’s update the RAG agent to summarize the tool call output immediately after it's received."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b0c1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_summarization_prompt = \"\"\"You will be provided a document from a RAG system.\n",
    "Summarize the document, ensuring to retain all relevant and essential information.\n",
    "Your goal is to reduce the size of the document (tokens) to a more manageable size for an agent.\"\"\"\n",
    "\n",
    "def tool_node_with_summarization(state: dict):\n",
    "    \"\"\"Performs the tool call and then immediately summarizes the output.\"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    result = []\n",
    "    for tool_call in last_message.tool_calls:\n",
    "        tool = rag_tools_by_name[tool_call[\"name\"]]\n",
    "        observation = tool.invoke(tool_call[\"args\"])\n",
    "        \n",
    "        # Summarize the document before adding it to the state\n",
    "        summary_msg = llm.invoke([\n",
    "            SystemMessage(content=tool_summarization_prompt),\n",
    "            (\"user\", str(observation))\n",
    "        ])\n",
    "        \n",
    "        result.append(ToolMessage(content=summary_msg.content, tool_call_id=tool_call[\"id\"]))\n",
    "    return {\"messages\": result}\n",
    "\n",
    "# Build the more efficient workflow\n",
    "efficient_agent_builder = StateGraph(MessagesState)\n",
    "efficient_agent_builder.add_node(\"llm_call\", rag_llm_call)\n",
    "efficient_agent_builder.add_node(\"Action\", tool_node_with_summarization)\n",
    "efficient_agent_builder.set_entry_point(\"llm_call\")\n",
    "efficient_agent_builder.add_conditional_edges(\"llm_call\", should_continue_rag, {\"Action\": \"Action\", END: END})\n",
    "efficient_agent_builder.add_edge(\"Action\", \"llm_call\")\n",
    "\n",
    "efficient_agent = efficient_agent_builder.compile()\n",
    "display(Image(efficient_agent.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f4g5h6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the same query with the efficient agent\n",
    "query = \"Why does RL improve LLM reasoning according to the blogs?\"\n",
    "result = efficient_agent.invoke({\"messages\": [(\"user\", query)]})\n",
    "\n",
    "console.print(\"\\n[bold green]Efficient Agent Conversation Flow:[/bold green]\")\n",
    "format_messages(result['messages'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i7j8k9l0",
   "metadata": {},
   "source": [
    "**Result:** This simple change can cut token usage by nearly half, making the agent far more efficient and cost-effective, demonstrating the power of on-the-fly context compression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m1n2o3p4",
   "metadata": {},
   "source": [
    "### Isolating Context: Sub-Agents and Sandboxing\n",
    "The final principle is **isolating** context. This involves splitting up the context to prevent different tasks or types of information from interfering with each other. This is crucial for complex, multi-step problems.\n",
    "\n",
    "![Fourth Component of CE](https://cdn-images-1.medium.com/max/1000/1*-b9BLPkLHkYsy2iLQIdxUg.png)\n",
    "\n",
    "We will look at two powerful isolation techniques:\n",
    "1.  **Sub-Agent Architectures:** Using multiple, specialized agents managed by a supervisor.\n",
    "2.  **Sandboxed Environments:** Executing code in a secure, isolated environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q5r6s7t8",
   "metadata": {},
   "source": [
    "#### Isolating Context using Sub-Agents Architecture\n",
    "\n",
    "A common way to isolate context is by splitting tasks across sub-agents. OpenAI's [Swarm](https://github.com/openai/swarm) library was designed for this \"separation of concerns,\" where each agent manages a specific sub-task with its own tools, instructions, and context window.\n",
    "\n",
    "> *Subagents operate in parallel with their own context windows, exploring different aspects of the question simultaneously.* - Anthropic\n",
    "\n",
    "LangGraph supports this through a **supervisor** architecture. The supervisor delegates tasks to specialized sub-agents, each running in its own isolated context window. Let’s build a supervisor that manages a `math_expert` and a `research_expert`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u9v0w1x2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import prebuilt agent creators\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph_supervisor import create_supervisor\n",
    "\n",
    "# --- 1. Define Tools for Each Agent ---\n",
    "def add(a: float, b: float) -> float:\n",
    "    \"\"\"Add two numbers.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "def web_search(query: str) -> str:\n",
    "    \"\"\"Mock web search function that returns FAANG company headcounts.\"\"\"\n",
    "    return (\n",
    "        \"Here are the headcounts for each of the FAANG companies in 2024:\\n\"\n",
    "        \"1. **Facebook (Meta)**: 67,317 employees.\\n\"\n",
    "        \"2. **Apple**: 164,000 employees.\\n\"\n",
    "        \"3. **Amazon**: 1,551,000 employees.\\n\"\n",
    "        \"4. **Netflix**: 14,000 employees.\\n\"\n",
    "        \"5. **Google (Alphabet)**: 181,269 employees.\"\n",
    "    )\n",
    "\n",
    "# --- 2. Create Specialized Agents ---\n",
    "# Each agent has its own tools and instructions, isolating its context\n",
    "math_agent = create_react_agent(\n",
    "    model=llm,\n",
    "    tools=[add, multiply],\n",
    "    name=\"math_expert\",\n",
    "    prompt=\"You are a math expert. Always use one tool at a time.\"\n",
    ")\n",
    "\n",
    "research_agent = create_react_agent(\n",
    "    model=llm,\n",
    "    tools=[web_search],\n",
    "    name=\"research_expert\",\n",
    "    prompt=\"You are a world class researcher with access to web search. Do not do any math.\"\n",
    ")\n",
    "\n",
    "# --- 3. Create Supervisor Workflow ---\n",
    "# The supervisor coordinates the agents\n",
    "supervisor_workflow = create_supervisor(\n",
    "    [research_agent, math_agent],\n",
    "    model=llm,\n",
    "    prompt=(\n",
    "        \"You are a team supervisor managing a research expert and a math expert. \"\n",
    "        \"Delegate tasks to the appropriate agent to answer the user's query. \"\n",
    "        \"For current events or facts, use research_agent. \"\n",
    "        \"For math problems, use math_agent.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Compile the multi-agent application\n",
    "multi_agent_app = supervisor_workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y3z4a5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Execute the Multi-Agent Workflow ---\n",
    "result = multi_agent_app.invoke({\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"what's the combined headcount of the FAANG companies in 2024?\"\n",
    "        }\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Format and display the results, showing the delegation in action\n",
    "format_messages(result['messages'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d8e9f0",
   "metadata": {},
   "source": [
    "#### Isolation using Sandboxed Environments\n",
    "Another powerful way to isolate context is to use a sandboxed execution environment. Instead of the LLM just calling tools via JSON, a `CodeAgent` can write and execute code in a secure sandbox. The results are then returned to the LLM.\n",
    "\n",
    "This keeps heavy data or complex state (like variables in a script) outside the LLM’s token limit, isolating it in the environment.\n",
    "\n",
    "The `langchain-sandbox` provides a secure environment for executing untrusted Python code using Pyodide (Python compiled to WebAssembly). We can add this as a tool to any LangGraph agent.\n",
    "\n",
    "**Note:** Deno is required. Install it from: https://docs.deno.com/runtime/getting_started/installation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g1h2i3j4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the sandbox tool and a prebuilt agent\n",
    "from langchain_sandbox import PyodideSandboxTool\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# Create a sandbox tool. allow_net=True lets it install packages if needed.\n",
    "sandbox_tool = PyodideSandboxTool(allow_net=True)\n",
    "\n",
    "# Create a ReAct agent equipped with the sandbox tool\n",
    "sandbox_agent = create_react_agent(llm, tools=[sandbox_tool])\n",
    "\n",
    "# Execute a query that the agent can solve by writing and running Python code\n",
    "result = await sandbox_agent.ainvoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what's 5 + 7?\"}]},\n",
    ")\n",
    "\n",
    "# Format and display the results\n",
    "format_messages(result['messages'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k5l6m7n8",
   "metadata": {},
   "source": [
    "#### State Isolation in LangGraph\n",
    "Finally, it's important to remember that the agent’s **runtime state object** is itself a powerful way to isolate context. By designing a state schema with different fields, you can control what the LLM sees.\n",
    "\n",
    "For example, one field (like `messages`) can be shown to the LLM on each turn, while other fields store information (like raw tool outputs or intermediate calculations) that remains isolated until a specific node needs to access it. You’ve seen many examples of this throughout this notebook, where we explicitly read from and write to specific fields of the state object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o9p0q1r2",
   "metadata": {},
   "source": [
    "### Summarizing Everything\n",
    "Let’s summarize what we have done so far:\n",
    "\n",
    "*   **Write:** We used LangGraph `StateGraph` to create a **\"scratchpad\"** for short-term memory and an `InMemoryStore` for long-term memory, allowing our agent to store and recall information.\n",
    "*   **Select:** We demonstrated how to selectively pull relevant information from the agent’s state and long-term memory. This included using Retrieval-Augmented Generation (`RAG`) to find specific knowledge and `langgraph-bigtool` to select the right tool from many options.\n",
    "*   **Compress:** To manage long conversations and token-heavy tool outputs, we implemented summarization. We showed how to compress `RAG` results on-the-fly to make the agent more efficient and reduce token usage.\n",
    "*   **Isolate:** We explored keeping contexts separate to avoid confusion by building a multi-agent system with a supervisor that delegates tasks to specialized sub-agents and by using sandboxed environments to run code.\n",
    "\n",
    "All these techniques fall under **“Contextual Engineering”** — a strategy to improve AI agents by carefully managing their working memory (`context`) to make them more efficient, accurate, and capable of handling complex, long-running tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}